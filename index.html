<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="style.css" >
	<title> Coded Bias</title>
</head>
<body>


	<h1> Coded Bias </h1>
	<div class = "paragraph">
	<p>
		The private and public sectors are increasingly turning to artificial intelligence (AI) systems and machine learning algorithms to automate simple and complex decision-making processes. The mass-scale digitization of data and the emerging technologies that use them are disrupting most economic sectors, including transportation, retail, advertising, and energy, and other areas. AI is also having an impact on democracy and governance as computerized systems are being deployed to improve accuracy and drive objectivity in government functions.
	</p>
	</div>
	<div class = "paragraph">
	<p>
		The availability of massive data sets has made it easy to derive new insights through computers. As a result, algorithms, which are a set of step-by-step instructions that computers follow to perform a task, have become more sophisticated and pervasive tools for automated decision-making. While algorithms are used in many contexts, we focus on computer models that make inferences from data about people, including their identities, their demographic attributes, their preferences, and their likely future behaviors, as well as the objects related to them.
	</p>
	</div>

	<hr>
	<h1>Implicit Bias  </h1>

	<div class = content>
		<div class = "images">
			<iframe width="600" height="320" src="https://www.youtube.com/embed/jZl55PsfZJQ">
			</iframe>
		</div>
		<div class = "contentText">
				The Focus of this website is implicit bias. Most of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based on given attributes, classifying pictures into different categories, or teaching a computer the best way to play PAC-MAN — what do we do when we are asked to base predictions of protected attributes according to anti-discrimination laws?
				<br> <br>
				How do we ensure that we do not embed racist, sexist, or other potential biases into our algorithms, be it explicitly or implicitly?
		</div>

	</div>

			<div class = content2>
		<div class = "contentText2">
				In Shalini Kantayya’s “Coded Bias” effectively brings to light a modern civil rights issue that can be proven with data—the bias within facial recognition programs, particularly against those who are not like the white men who originally formed such technology. (Such powerful data collecting and distributing technology is used in America by the likes of Amazon, Google, and Microsoft.) But Kantayya’s doc expands this even further, discussing the invasiveness of this technology around the world, and the harm and misinformation it can lead to for people of color in America. <br> <br>

				The hero of this documentary is Joy Buolamwini, the Ghanian-American founder of the Algorithmic Justice League. She starts "Coded Bias" by showing a massive discovery that launched dozens of articles, and lead to her speaking at Congress, inspiring activism against this technology. As she sits in her office at MIT, she tells of how she discovered this facial recognition problem, in which the AI did not recognize her face. But when she put on a white mask, it did. <br> <br>

				The importance of this is enormous, and director Kantayya spends a fleet 85 minutes detailing why, without losing focus. As facial recognition technology becomes such a global problem, it comes with this harmful bias against people of color, informed by conscious or unconscious biases from those who created such algorithms. It’s not uncommon for this technology to correctly identify a white face, but then to give the wrong information about someone of darker complexion. On top of that, the talking heads in this documentary (mostly women) express how the algorithms themselves are a type of black box, in which we don’t exactly know what they're thinking, aside from the copious data they contain. We also don't know what these black boxes are entirely capable of. 
		</div>
	</div>

	<hr>

	<h1>How Can Data be Biased? </h1>

	<div class = content>
		<div class = "images">
			<img class = "img" src="https://assets.weforum.org/editor/dRPuO8eQbLBo4qooHzFf0E6gPvwmtEzlZT6Fv3cBN_0.jpg">
		</div>
		<div class = "contentText">
				The National Institute of Standards and Technology (NIST) conducted research that evaluated facial-recognition algorithms from around 100 developers from 189 organizations, including Toshiba, Intel and Microsoft. Speaking about the alarming conclusions, one of the authors, Patrick Grother, says: "While it is usually incorrect to make statements across algorithms, we found empirical evidence for the existence of demographic differentials in the majority of the algorithms we studied.” <br> 
				<br>
				To list some of the source of fairness and non-discrimination risks in the use of artificial intelligence, these include: implicit bias, sampling bias, temporal bias, over-fitting to training data, and edge cases and outliers.
		</div>
	</div>

	<div class = content2>
		<div class = "contentText2">
				Discrimination impacts social goods when classification and decision making is based on inaccurate information (for example, thinking that everyone over 7ft is a bad babysitter). These ideas are often perpetuated by human biases, and become embedded in data that is used to train algorithms. <br> <br>

				In this case, the biases of humans are not mitigated by the machine learning algorithm. In fact, they are reproduced in the classifications that are made. Why does this happen? Recidivism scores such as those made by the Northpointe software are based on prior arrests, age of first police contact, parents’ incarceration record. This information is shaped by biases in the world (such as from cultural values and nationalism) and injustices more generally (such as racial prejudices). <br> <br>

				This bias is also present in natural language processing, which focuses on textual data. A good example of this is the research paper titled “Man is to Computer Programmer as Women is to Homemaker? Debiasing Word Embeddings”, which showed automatically generated analogies from the software’s vectors, such as man → computer-programmer, and women → homemaker. These reflect sexism in the original texts.
		</div>
	</div>

	<hr>

	<h1> Conclusions </h1>	

	<div class = content>
		<div class = "images">
			<img class = "img" src="https://static01.nyt.com/images/2020/01/03/multimedia/20sp-women-ai1-print/20sp-women-ai1a-videoSixteenByNineJumbo1600.jpg">
		</div>
		<div class = "contentText">
				More generally, bias can emerge from:
					<li>Over- and under-sampling</li>
					<li>Skewed sample </li>
					<li>Feature choice/limited features</li>
					<li>Proxies/redundant encodings</li>
					<li>Biases and injustices in the world</li>
				<br>
				So how do we remove these biases? Machine learning algorithms can perpetuate discrimination because they are trained on biased data. The solution is to identify or generate an unbiased dataset from which to draw accurate generalizations. As future designers and engineers, we must take steps to create and collect data that are equal and unbaised to our best efforts. Without that, the vicuous cycle will continue and algorithms will never be fair.
		</div>
	</div>

	<hr>

	<h2> Resources <h2>
	<div class = "contentText2">
		<li><a href="https://towardsdatascience.com/machine-learning-and-discrimination-2ed1a8b01038">Handling Discriminatory Biases in Data for Machine Learning</a></li>
		<li><a href="https://www.codedbias.com/"> Code Bias Movie</a></li>
		<li><a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0177-4"> Big Data and Discrimination</a></li>
		<li><a href="https://www.weforum.org/agenda/2021/07/ai-machine-learning-bias-discrimination/">World Economic Fourm article</a></li>	
	</div>


</body>
</html>
